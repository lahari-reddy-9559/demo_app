{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOkBnpOIU8mJHKEJ1rSFv5F"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "uAYBMoWA_47n"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKo6NGVw7lvC",
        "outputId": "b29b2fb0-6a75-4c51-9454-f7be25597193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers sentencepiece torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16GfjrW870qX",
        "outputId": "bf21e692-3f0b-4de9-ecf0-2948bed42d58"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import re\n",
        "import math\n",
        "import argparse\n",
        "import heapq\n",
        "import json\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    from transformers import pipeline, AutoTokenizer\n",
        "    import torch\n",
        "except Exception:\n",
        "    pipeline = None\n",
        "    AutoTokenizer = None\n",
        "    torch = None\n",
        "    _TRANSFORMERS_IMPORT_ERROR = None\n",
        "STOPWORDS = {\n",
        "    \"a\", \"an\", \"the\", \"and\", \"or\", \"but\", \"if\", \"while\", \"with\", \"without\",\n",
        "    \"to\", \"from\", \"by\", \"for\", \"of\", \"on\", \"in\", \"at\", \"is\", \"are\", \"was\", \"were\",\n",
        "    \"this\", \"that\", \"these\", \"those\", \"it\", \"its\", \"be\", \"as\", \"which\", \"not\",\n",
        "}\n",
        "\n",
        "_SENT_SPLIT_RE = re.compile(r'(?<=[.!?])\\s+')\n",
        "\n",
        "def split_sentences(text: str) -> List[str]:\n",
        "    sents = [s.strip() for s in _SENT_SPLIT_RE.split(text) if s.strip()]\n",
        "    return sents or [text.strip()]\n",
        "\n",
        "def word_tokens(text: str) -> List[str]:\n",
        "    return [w.lower() for w in re.findall(r\"\\w+\", text) if w.lower() not in STOPWORDS]\n",
        "\n",
        "def extractive_reduce(text: str, ratio: float = 0.3, min_sentences: int = 1, max_sentences: int = 8) -> str:\n",
        "\n",
        "    sentences = split_sentences(text)\n",
        "    if len(sentences) <= 1:\n",
        "        return text\n",
        "\n",
        "    freq = {}\n",
        "    for sent in sentences:\n",
        "        for w in word_tokens(sent):\n",
        "            freq[w] = freq.get(w, 0) + 1\n",
        "\n",
        "    scores = []\n",
        "    for i, sent in enumerate(sentences):\n",
        "        s = sum(freq.get(w, 0) for w in word_tokens(sent))\n",
        "        scores.append((s, i, sent))\n",
        "\n",
        "    keep = max(min_sentences, min(max_sentences, math.ceil(len(sentences) * ratio)))\n",
        "    top = heapq.nlargest(keep, scores, key=lambda x: (x[0], -x[1]))\n",
        "    top_sorted = sorted(top, key=lambda x: x[1])\n",
        "    reduced = \" \".join([s for (_score, _i, s) in top_sorted])\n",
        "    return reduced\n",
        "\n",
        "def make_abstractive_pipeline(model_name: str = \"t5-small\"):\n",
        "    if pipeline is None:\n",
        "        raise RuntimeError(\"transformers/torch not installed. Install: pip install transformers sentencepiece torch\")\n",
        "    device = 0 if torch and torch.cuda.is_available() else -1\n",
        "    return pipeline(\"summarization\", model=model_name, tokenizer=model_name, device=device)\n",
        "\n",
        "def trim_for_model(text: str, model_name: str, fraction_of_model_max: float = 0.9) -> str:\n",
        "    if AutoTokenizer is None:\n",
        "        return text\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_max = getattr(tokenizer, \"model_max_length\", 512) or 512\n",
        "    if model_max > 16384:\n",
        "        model_max = 1024\n",
        "    budget = max(64, int(model_max * fraction_of_model_max))\n",
        "\n",
        "    sentences = split_sentences(text)\n",
        "    if not sentences:\n",
        "        return text\n",
        "\n",
        "    def token_count(s: str) -> int:\n",
        "        ids = tokenizer.encode(s, add_special_tokens=False, truncation=False)\n",
        "        return len(ids)\n",
        "\n",
        "    joined = \" \".join(sentences)\n",
        "    if token_count(joined) <= budget:\n",
        "        return joined\n",
        "\n",
        "\n",
        "    left = 0\n",
        "    right = len(sentences) - 1\n",
        "    while left <= right:\n",
        "        candidate = sentences[:left + 1] + sentences[right:]\n",
        "        if token_count(\" \".join(candidate)) <= budget:\n",
        "            return \" \".join(candidate)\n",
        "\n",
        "        right -= 1\n",
        "        if right < left:\n",
        "            break\n",
        "\n",
        "\n",
        "    first = sentences[0]\n",
        "    ids = tokenizer.encode(first, add_special_tokens=False)\n",
        "    ids = ids[:max(1, budget)]\n",
        "    return tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "\n",
        "def abstractive_summarize_text(text: str, model_name: str = \"t5-small\",\n",
        "                               max_length: int = 120, min_length: int = 20,\n",
        "                               use_extractive_reduced: bool = True) -> str:\n",
        "    if pipeline is None:\n",
        "        raise RuntimeError(\"transformers not installed. Install: pip install transformers sentencepiece torch\")\n",
        "\n",
        "\n",
        "    if use_extractive_reduced:\n",
        "        reduced = extractive_reduce(text, ratio=0.25, min_sentences=1, max_sentences=8)\n",
        "    else:\n",
        "        reduced = text\n",
        "\n",
        "\n",
        "    trimmed = trim_for_model(reduced, model_name)\n",
        "    summarizer = make_abstractive_pipeline(model_name)\n",
        "    out = summarizer(trimmed, max_length=max_length, min_length=min_length, do_sample=False)\n",
        "    if isinstance(out, list) and out:\n",
        "        return out[0].get(\"summary_text\", \"\").strip()\n",
        "    return str(out)\n",
        "\n",
        "def read_input_text(args) -> str:\n",
        "    if args.text:\n",
        "        return args.text\n",
        "    if args.input_file:\n",
        "        with open(args.input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            return f.read()\n",
        "    if not sys.stdin.isatty():\n",
        "        return sys.stdin.read()\n",
        "    return \"\"\n",
        "\n",
        "def main(args):\n",
        "    text = read_input_text(args)\n",
        "    if not text or not text.strip():\n",
        "        print(\"No input text provided. Use --text, --input-file, or pipe text via stdin.\", file=sys.stderr)\n",
        "        sys.exit(2)\n",
        "\n",
        "    extractive = None\n",
        "    abstractive = None\n",
        "\n",
        "    if not args.no_extractive:\n",
        "        extractive = extractive_reduce(text, ratio=args.ratio)\n",
        "    if not args.no_abstractive:\n",
        "        try:\n",
        "\n",
        "            if args.no_gpu and torch is not None:\n",
        "\n",
        "                import os\n",
        "                os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "            abstractive = abstractive_summarize_text(text, model_name=args.model,\n",
        "                                                     max_length=args.max_len, min_length=args.min_len,\n",
        "                                                     use_extractive_reduced=not args.use_full_for_abstractive)\n",
        "        except Exception as e:\n",
        "            abstractive = None\n",
        "            print(f\"[warning] abstractive step failed: {e}\", file=sys.stderr)\n",
        "    if extractive is not None:\n",
        "        print(\"=== Extractive Summary ===\")\n",
        "        print(extractive)\n",
        "        print()\n",
        "\n",
        "    if abstractive is not None:\n",
        "        print(\"=== Abstractive (Generative) Summary ===\")\n",
        "        print(abstractive)\n",
        "        print()\n",
        "    if args.json_out:\n",
        "        out = {\"extractive\": extractive, \"abstractive\": abstractive}\n",
        "        with open(args.json_out, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(out, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "iUYLhlld71ks"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = input(\"Please enter the text you want to summarize: \")\n",
        "main(argparse.Namespace(text=input_text, input_file=None, model=\"t5-base\", max_len=120, min_len=20, ratio=0.5, no_extractive=False, no_abstractive=False, use_full_for_abstractive=False, json_out=None, no_gpu=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aAib_49h7-j1",
        "outputId": "01fbb976-4963-4d4b-db9a-488db4a3a13d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter the text you want to summarize: This product is a complete sham. The wireless charging has zero reliability. Don't purchase it just because of the Spigen brand attached to it. Had thoughtvthat this would be a novel charging solution, but it fails on all fronts - desk charging, wireless lortable charging and enen the 2 in 1 charging. Never buying Spigen products again.A not much space taking and elegant product, yet wireless charging not that successful as it heats up devices very soon. Prices could be a little less. fast charging is also a little slow while using a Samsung phone and comparing the same with a Samsung power bank.Wireless charging not working On off switch not able use easily Overall waste of money, Don’t buy this product.Pros:- 1. Works as a Multiple Wireless Charger, Charging Iphone, Airpods & Watch all at once while plugged in with 20W Apple adapter. 2. Works as a Powerbank to Fastcharge with both wired Type A (18W) & Type C (20W) Capabilities.Cons:- 1. Powerbank is heavy 2. The power bank should have a strong magnet or magsafe compatible to keep it securely in place on the charging pad, so it doesn't move easily.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Extractive Summary ===\n",
            "Had thoughtvthat this would be a novel charging solution, but it fails on all fronts - desk charging, wireless lortable charging and enen the 2 in 1 charging. Never buying Spigen products again.A not much space taking and elegant product, yet wireless charging not that successful as it heats up devices very soon. fast charging is also a little slow while using a Samsung phone and comparing the same with a Samsung power bank.Wireless charging not working On off switch not able use easily Overall waste of money, Don’t buy this product.Pros:- 1. Works as a Multiple Wireless Charger, Charging Iphone, Airpods & Watch all at once while plugged in with 20W Apple adapter. Works as a Powerbank to Fastcharge with both wired Type A (18W) & Type C (20W) Capabilities.Cons:- 1. The power bank should have a strong magnet or magsafe compatible to keep it securely in place on the charging pad, so it doesn't move easily.\n",
            "\n",
            "=== Abstractive (Generative) Summary ===\n",
            "not much space taking and elegant product, yet wireless charging not that successful as it heats up devices very soon . fast charging also a little slow while using a Samsung phone and comparing the same with a .\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "checking for halucination"
      ],
      "metadata": {
        "id": "iZ9_m8Gk8ciz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"The quick brown fox jumps over the lazy dog. This is a test sentence to check for hallucination.\"\n",
        "abstractive_summary = abstractive_summarize_text(input_text)\n",
        "extractive_summary = extractive_reduce(input_text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(input_text)\n",
        "print(\"\\nAbstractive Summary:\")\n",
        "print(abstractive_summary)\n",
        "print(\"\\nExtractive Summary:\")\n",
        "print(extractive_summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXoYMewo8L0j",
        "outputId": "6627de9c-5b3c-4ace-c8c9-307d5ee16359"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 120, but your input_length is only 15. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=7)\n",
            "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "The quick brown fox jumps over the lazy dog. This is a test sentence to check for hallucination.\n",
            "\n",
            "Abstractive Summary:\n",
            "the quick brown fox jumps over the lazy dog . the quick fox is a fox that can jump over the fox .\n",
            "\n",
            "Extractive Summary:\n",
            "The quick brown fox jumps over the lazy dog.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "if halucinating:\n",
        "the quick fox is a fox that can jump over the fox\n",
        "\n",
        "Compare with different models\n",
        "Fine-tune the model"
      ],
      "metadata": {
        "id": "n6h4m3Vc8kdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "using some of the metrics here in transformers.\n",
        "we have options such as T5-small , t5-base ,bert in my model I am using t5-base and t5-small"
      ],
      "metadata": {
        "id": "vYueVl7V9SOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J55PcAg8f6r",
        "outputId": "23e0ca3b-9324-42ed-ba6d-900c3c5f4224"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "scores = scorer.score(abstractive_summary, input_text)\n",
        "\n",
        "print(\"\\nROUGE Scores (Abstractive Summary vs. Original Text):\")\n",
        "for key, value in scores.items():\n",
        "    print(f\"{key}: {value}\")\n",
        "\n",
        "scores_extractive = scorer.score(extractive_summary, input_text)\n",
        "print(\"\\nROUGE Scores (Extractive Summary vs. Original Text):\")\n",
        "for key, value in scores_extractive.items():\n",
        "    print(f\"{key}: {value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBPnOcqS8uo6",
        "outputId": "9d2b751f-568e-414b-f5ec-fd65dcda176e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores (Abstractive Summary vs. Original Text):\n",
            "rouge1: Score(precision=0.6111111111111112, recall=0.5238095238095238, fmeasure=0.5641025641025642)\n",
            "rouge2: Score(precision=0.5294117647058824, recall=0.45, fmeasure=0.48648648648648646)\n",
            "rougeL: Score(precision=0.6111111111111112, recall=0.5238095238095238, fmeasure=0.5641025641025642)\n",
            "\n",
            "ROUGE Scores (Extractive Summary vs. Original Text):\n",
            "rouge1: Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)\n",
            "rouge2: Score(precision=0.47058823529411764, recall=1.0, fmeasure=0.6399999999999999)\n",
            "rougeL: Score(precision=0.5, recall=1.0, fmeasure=0.6666666666666666)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "reference = [word_tokenize(input_text)]\n",
        "hypothesis = word_tokenize(abstractive_summary)\n",
        "\n",
        "bleu_score_abstractive = sentence_bleu(reference, hypothesis)\n",
        "\n",
        "print(\"\\nBLEU Score (Abstractive Summary vs. Original Text):\")\n",
        "print(bleu_score_abstractive)\n",
        "\n",
        "hypothesis_extractive = word_tokenize(extractive_summary)\n",
        "\n",
        "bleu_score_extractive = sentence_bleu([word_tokenize(input_text)], hypothesis_extractive)\n",
        "\n",
        "print(\"\\nBLEU Score (Extractive Summary vs. Original Text):\")\n",
        "print(bleu_score_extractive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQZzrjT_8yfH",
        "outputId": "2d7ae899-6f00-44c2-b1f4-72619065a4ee"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BLEU Score (Abstractive Summary vs. Original Text):\n",
            "0.38222431380970806\n",
            "\n",
            "BLEU Score (Extractive Summary vs. Original Text):\n",
            "0.36787944117144233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "forFyj7MA0xo"
      },
      "execution_count": 37,
      "outputs": []
    }
  ]
}